<html>
<head>
  <title>BISICLES site specific notes</title> 
  <link href="bikedoc.css" rel="stylesheet" type="text/css"/>
</head>
<body>

<div id="head">
<ul>
<li><a href='index.html'>Index page<a/> </li>
<li><a href='#top'>Top of page<a/> </li>
</ul>

<h1>Contents</h1>

<ol>
  <li><a href='#debian'>Debian based systems (e.g Ubuntu) </a></li>
  <li><a href='#uob'>University of Bristol</a></li>
  <li><a href='#nersc'>NERSC</a></li>
  <li><a href='#archer'>ARCHER (Cray XC30,</li>
  <li><a href='#monsoon'>UKMO Monsoon (Cray XC40)</a></li>
  <li><a href='#hpcwales'>HPC Wales (Swansea University) </a></li>
  <li><a href='#monsoonaix'>UKMO Monsoon (AIX, defunct)</li>
</ol>
</div>


<div id="main">
<h1><a name='top'>BISICLES site specific notes</a></h1>

<h2><a name='debian'>Debian based systems (e.g Ubuntu)</a></h2>

<p>
  Recent Debian GNU/Linux derivatives (from around 2016), including Ubuntu 18.04 (the latest LTS) allow an mpi environment ,
  hdf5, and netcdf to be installed in a way that works well for BISICLES. On Ubuntu run
</p>
<pre>
  sudo apt-get install subversion g++ gfortran csh mpi-default-bin mpi-default-dev libhdf5-mpi-dev libhdf5-dev hdf5-tools libnetcdff-dev libnetcdf-dev python libpython-dev
</pre>
<p>
  The file $BISICLES_HOME/Chombo/lib/mk/Make.defs.local should contain:
</p>
<pre>
PRECISION     = DOUBLE  
CXX           = g++
FC            = gfortran
MPICXX        = mpiCC
USE_HDF       = TRUE
HDFINCFLAGS   = -I/usr/include/hdf5/serial/
HDFLIBFLAGS   = -L/usr/lib/x86_64-linux-gnu/hdf5/serial/ -lhdf5 -lz
HDFMPIINCFLAGS= -I/usr/include/hdf5/openmpi/ 
HDFMPILIBFLAGS= -L/usr/lib/x86_64-linux-gnu/hdf5/openmpi/ -lhdf5  -lz
cxxdbgflags    = -g -fPIC 
cxxoptflags    = -fPIC -O2
fdbgflags     =  -g -fPIC 
foptflags     = -fPIC -O3 -ffast-math -funroll-loops 
</pre>
<p>
  and the file $BISICLES_HOME/BISICLES/code/mk/Make.defs.$mymachine (where $mymachine is
  the output from 'uname -n') should contain
</p>
<pre>
NETCDF_HOME=$(shell nc-config --prefix)
NETCDF_LIBS=-lnetcdff -lnetcdf -lhdf5_hl
PYTHON_VERSION=2.7
</pre>


<h2><a name='uob'>University of Bristol</a>

  <h3>Blue Crystal Phase 3</h3>

  You might not need to compile BISICLES on Blue Crystal Phase 3, since
  a number of the the developers work there. 

  <h4>Blue Crystal Phase 3, GNU Compilers</h4>
  <p>
    To use the GNU compilers on Blue Crystal Phase 3, include
  </p>
  <pre>
    module load languages/gcc-7.1.0
    module load openmpi/gcc/64/2.1.1
    module load languages/python-2.7.5
  </pre>
  <p>
    in .bash_profile. There are ready-made Make.defs.newblue files
    that work with the python install, but hdf5 (and netcdf) need to
    be built as described in the <a href='readme.html'>generic instructions</a>.
    
    <h4>Blue Crystal Phase 3, Intel Compilers</h4>
  <p>
    To use the Intel compilers on Blue Crystal Phase 3, include
  </p>
  <pre>
    module load intel-cluster-studio/impi/64
    module load intel-cluster-studio/compiler/64
    module load openmpi/intel/64
    module load languages/python-2.7.5
  </pre>
  <p>
    in .bash_profile. There are ready-made Make.defs.newblue files
    that work with the python install, but hdf5 (and netcdf) need to
    be built as described in the <a href='readme.html'>generic instructions</a>.
    
    <h2><a name='nersc'>NERSC</a></h2>
    The US DOE-run National Energy Research Supercomputing Center ( <a href='http://www.nersc.gov'>NERSC</a>) has several machines which BISICLES users may find useful. 
    
<h3>Edison -- GNU compilers</h3>

<p>
  The Cray XC30 (Edison) at NERSC offers Cray, GNU, and Intel compilers. All three compilers can be supported with a single
  Make.defs.local, it only only necessary to load the correct PrgEnv module and the cray-hdf5-parallel.
</p>

<p>
  To compile with GNU, switch to the correct PrgEnv and load an hdf5 module, plus netcdf and python if desired
</p>
<pre>
> module swap PrgEnv-cray PrgEnv-gnu
> module load cray-hdf5-parallel
> module load python
> module load cray-netcdf-hdf5parallel
> module unload  cray-shmem #needed for python only
</pre>
<p>
  To have the python interface work, the compiler needs the -shared and -fPIC flags, and
  the linker needs the -dynamic flag. You may also need to have the environment variable CRAYPE_LINK_TYPE set to "dynamic"; in the csh or tcsh shells:
</p>
<pre>
> setenv CRAYPE_LINK_TYPE dynamic
</pre>

 You should be able to use the existing Chombo/lib/mk/loocal/Make.defs.edison file in the Chombo release as a starting point.  An example $BISICLES_HOME/Chombo/lib/mk/local/Make.defs.local would be:
</p>
<pre>
makefiles+=Make.defs.local

#default to MPI=TRUE,OPT=TRUE,DEBUG=FALSE 
MPI=TRUE
OPT=TRUE
DEBUG=FALSE

#this seems to be the Cray way
CXX=CC
FC=ftn
MPICXX=CC
USE_64=TRUE

ifeq ($(PE_ENV),GNU)

CH_CPP=$(CXX) -E -P 
XTRACONFIG=.GNU
cxxoptflags += -shared -fPIC
foptflags += -shared -fPIC
ldoptflags += -dynamic
cxxoptflags += -march=ivybridge -O2 -mavx -ftree-vectorize -ffast-math -funroll-loops
foptflags += -march=ivybridge -O2 -mavx -ftree-vectorize -ffast-math -funroll-loops

else
$(ECHO) "UNKNOWN PROGRAMMING ENVIRONMENT!"
endif

# The appropriate module (cray-hdf5-parallel) must be loaded for this to work.
USE_HDF=TRUE
HDFLIBFLAGS=   -L$(HDF5_DIR)/lib     $(HDF_POST_LINK_OPTS)  -lhdf5 -lz
HDFMPILIBFLAGS=-L$(HDF5_DIR)/lib     $(HDF_POST_LINK_OPTS)  -lhdf5 -lz
HDFINCFLAGS=   -I$(HDF5_DIR)/include $(HDF_INCLUDE_OPTS)
HDFMPIINCFLAGS=-I$(HDF5_DIR)/include $(HDF_INCLUDE_OPTS)
</pre>

<p>
  For python and netcdf, $BISICLES_HOME/BISICLES/code/mk/Make.defs.edison should include:
</p>
<pre>
PYTHON_INC=/usr/common/usg/python/2.7.9/include/python2.7
PYTHON_LIBS=-L/usr/common/usg/python/2.7.9/lib/ -lpython2.7 -ldl -lutil -lm -Xlinker -export-dynamic
NETCDF_INC=$(shell nc-config --includedir)
NETCDF_LIBS=$(shell nc-config --flibs)

NETCDF_HOME =$(NETCDF_DIR)
NETCDF_LIBS= -lnetcdff -lnetcdf
PYTHON_VERSION=2.7
PYTHON_INC=-I$(PYTHON_DIR)/include/python2.7
PYTHON_LIBS=-L$(PYTHON_DIR)/lib/ -lpython2.7 -ldl -lutil -lm -Xlinker -export-dynamic
NETCDF_INC=$(shell nc-config --includedir)
NETCDF_LIBS=$(shell nc-config --flibs)
</pre>
  
<h3>Hopper -- GNU compilers</h3>

The admiral has now been dismantled, and it was pretty similar to another Cray, <a href='#archer'>ARCHER</a>,
so we will say no more.

<h2><a name='archer'>ARCHER</a></h2>

<p>
The Cray XC30 at ARCHER (UK National HPC) offers Cray, GNU, and Intel compilers.
Cray is the default (but doesn't seem to work for now). All three compilers can be supported with a single
Make.def.locals, it only only necessary to load the correct PrgEnv module and the cray-hdf5-parallel. 
One complexity is with statically versus  dynamically linked executables. Static linking
is often simpler, but, will prevent you from using the python interface as freely as you might
like - in particular, 'import math', which is needed for many common math functions, will
result in a segfault. The alternative is dynamic linking, which results in executables
that have a longer list of dependencies that must be matched between login nodes and compute
nodes. Choose static if you have no strong preference.
</p>

  <p>
    It is not generally a good idea to work with the filetools (nctoamr, stats, flatten etc)
    on ARCHER - these are intended for interactive use, and can be used on a workstation
    even with larger data, such as whole Antarctic simulations. Ideally, you
    will have a GNU/linux workstation to compile and run these tools, but
    if you have only a Windows or Mac OS X machine, one possibility is to run
    a virtual machine with GNU/linux. We have found that Oracle virtualbox
    and Ubuntu 18.04 work well on both Windows and MacOS X machines. 
  </p>
  
<h3>ARCHER -- GNU compilers -- static linking </h3>

  <p>
    For static linking, use the file Make.defs.archer included with Chombo
  </p>
  <pre>
    ln -s $BISICLES_HOME/Chombo/lib/mk/local/Make.defs.archer $BISICLES_HOME/Chombo/lib/mk/Make.defs.local
  </pre>
  
  <p>
    To build statically linked executables with GNU, switch to the correct PrgEnv and load an hdf5 module
    q</p>
  <pre>
    > module unload PrgEnv-cray PrgEnv-intel
    > module load PrgEnv-gnuq
    > module load cray-hdf5-parallel
    > module load cray-petsc # if you want to build petsc-enabled executables
    > module load cray-netcdf-hdf5parallel # if you want to build the filetools
  </pre>

  <p>
    $BISICLES_HOME/BISICLES/code/mk/Make.defs.archer
    includes the following details for the python interface
    and netcdf. 
  </p>
  <pre>   
    PYTHON_DIR=/work/y07/y07/cse/python/2.7.6-static/
    PYTHON_INC=-I$(PYTHON_DIR)/include/python2.7/
    PYTHON_LIBS=-L$(PYTHON_DIR)/lib -lpython2.7 -lpthread -ldl -lutil
    
    NETCDF_INC=-I$(NETCDF_DIR)/include
    NETCDF_LIBS=-L$(NETCDF_DIR)/lib -lnetcdf
  </pre>

<p>
  Copy this file to Make.defs.none or Make.defs.$UNAMEN
  (where $UNAMEN is the output from uname -n)
  Then compile:
</p>
<pre>
  cd BISICLES/code/exec2D
  make all OPT=TRUE MPI=TRUE DEBUG=FALSE # are all default 
</pre>
  <p>
    the exectable will be driver2d.Linux.64.CC.ftn.OPT.MPI.GNU.ex. To
    compile a PETSc-enable executable,  driver2d.Linux.64.CC.ftn.OPT.MPI.PETSC.GNU.ex
  </p>
  <pre>
     cd BISICLES/code/exec2D
     make all OPT=TRUE MPI=TRUE DEBUG=FALSE USE_PETSC=TRUE 
  </pre>

  <h3>ARCHER -- GNU compilers -- dynamic linking </h3>
  
  <p>
    For dynamic linking, use the file Make.defs.archer_dynamic_chombo included with BISICLES
  </p>
  <pre>
    ln -s $BISICLES_HOME/BISICLES/code/mk/Make.defs.archer_dynamic_chombo $BISICLES_HOME/Chombo/lib/mk/Make.defs.local
  </pre>
  
  <p>
To build with GNU, switch to the correct PrgEnv and load an hdf5 module
  </p>
  <pre>
    > module unload PrgEnv-cray PrgEnv-intel
    > module load PrgEnv-gnuq
    > module load cray-hdf5-parallel
    > module load python-compute
    > module load cray-netcdf-hdf5parallel
    > module load cray-petsc #if you want petsc
    > module unload cray-shmem
    > export CRAYPE_LINK_TYPE dynamic
  </pre>
  
  <p>
    $BISICLES_HOME/BISICLES/code/mk/Make.defs.archer_dyamic
    includes the following details for the python interface
    and netcdf. 
</p>
  <pre>
    PYTHON_DIR=/work/y07/y07/cse/python/2.7.6/
    PYTHON_INC=-I$(PYTHON_DIR)/include/python2.7
    PYTHON_LIBS=-L$(PYTHON_DIR)/lib -lpython2.7 -lm -lpthread -lutil 

    NETCDF_INC=-I$(NETCDF_DIR)/include
    NETCDF_LIBS=-L$(NETCDF_DIR)/lib -lnetcdf
</pre>

  <p>
    Copy this file to Make.defs.none or Make.defs.$UNAMEN
    (where $UNAMEN is the output from uname -n)
    Then compile 
  </p>
  <pre>
    cd BISICLES/code/exec2D
    make all OPT=TRUE MPI=TRUE DEBUG=FALSE # are all default 
  </pre>
  <p>
    the exectable will be driver2d.Linux.64.CC.ftn.OPT.MPI.GNU.DY.ex. To
    compile a PETSc-enable executable,  driver2d.Linux.64.CC.ftn.OPT.MPI.PETSC.GNU.DY.ex
  </p>
  <pre>
    cd BISICLES/code/exec2D
    make all OPT=TRUE MPI=TRUE DEBUG=FALSE USE_PETSC=TRUE 
  </pre>
  </p>

  
<h3>ARCHER - Intel compilers </h3>

Compiling with Intel is essentially the same as with GNU. It's just a case of running
<pre>
  > module unload PrgEnv-cray PrgEnv-gnu
  > module load PrgEnv-gnu
  > module load cray-hdf5-parallel # and so on
</pre>
instead of
<pre>
  > module unload PrgEnv-cray PrgEnv-intel
  > module load PrgEnv-gnu
  > module load cray-hdf5-parallel #and so on
</pre>
<p>
The binary will be called driver2d.Linux.64.CC.ftn.OPT.MPI.INTEL.ex. (or driver2d.Linux.64.CC.ftn.OPT.MPI.INTEL.DY.ex)
</p>
<h3>Running jobs on ARACHER</h3>
  
<p> ARCHER's compute nodes do not have access to the /home mount point (and therefore your home directory) 
It's not obvious where login scripts come from in that case, either. This can all be dealt with inside the submission script
(the MOM nodes do have access to home...). A minimal script (run from somewhere in /work) would look something like
<pre>
#PBS -l walltime=00:10:00 
#PBS -j oe 
#PBS -l select=1
#PBS -A /your allocation/  

export PBS_O_WORKDIR=$(readlink -f $PBS_O_WORKDIR)

#load modules needed to get GLIBCXX, HDF5, python where the compute nodes can see them
module unload PrgEnv-cray PrgEnv-intel
module load PrgEnv-gnu
module load cray-hdf5-parallel
module load python-compute

cd $PBS_O_WORKDIR

#A GNU dymamic executable - change the name for intel, static linking, etc  
EXE=/path/to/driver2d.Linux.64.CC.ftn.OPT.MPI.GNU.DY.ex 
aprun -n 24 $EXE inputs.whatever
</pre>

<h2><a name='monsoon'>The Monsoon Cray XC40 at the UK Met Office</a></h2>
<p>
  The Monsoon Cray XC40 is much like ARCHER (and eidison and cori at NERSC).
  So much so that the ARCHER makefile seems to work, ie
</p>
<pre>
ln -s $BISICLES_HOME/Chombo/lib/mk/local/Make.defs.archer $BISICLES_HOME/Chombo/lib/mk/Make.defs.local
</pre>
<p>
  I'm assuming that Monsoon users are interested in bisicles coupled via glimmer-cism to
  UKESM, i.e have checked out UniCiCles, and want to use the intel compiler. 
</p>
<pre>
> module swap PrgEnv-cray PrgEnv-intel
> module load cray-hdf5-parallel
> module load python/v2.7.9 #optional, needed if you want the python interface
> module load cray-netcdf-hdf5parallel #optional, needed if you want glimmer-cism 
> module load cray-tpsl/1.5.2 #optional, needed if you want the petsc solver
> module load cray-petsc/3.6.1.0 #optional, needed if you want the petsc solver
</pre>
<p>
If you want the python interface, make sure that  $BISICLES_HOME/BISICLES/code/mk/Make.defs includes 
the following
</p>
<pre>      
PYTHON_DIR=opt/python/gnu/2.7.9                                                                                    
PYTHON_INC=$(PYTHON_DIR)/include/python2.7                                                                        
PYTHON_LIBS=-L$(PYTHON_DIR)/lib -lpython2.7                                                            
</pre>
<p>
and for netcdf stuff (e.g glimmer-cism, filetoools)
</p>
NETCDF_INC=$(NETCDF_DIR)/include  
NETCDF_LIBS=-L$(NETCDF_DIR)/lib -lnetcdf
<p>
Compile with
</p>
<pre>
cd BISICLES/code/exec2D
make all OPT=TRUE MPI=TRUE DEBUG=FALSE # are all default 
</pre>
<op>
The binary will be called driver2d.Linux.64.CC.ftn.OPT.MPI.INTEL.ex
</p>
<p>
Or, with the PETSC interface
</p>
<pre>
cd BISICLES/code/exec2D
make all OPT=TRUE MPI=TRUE DEBUG=FALSE USE_PETSC=TRUE
</pre>
<op>
The binary will be called driver2d.Linux.64.CC.ftn.OPT.MPI.PETSC.INTEL.ex
</p>

<h2><a name='hpcwales'>HPC Wales (Swansea University)</a></h2>
<p>
  HPC Wales providea number of clusters, including two at Swansea University. To login to the Swansea
  systems, first login to the HPC Wales login node, then a Swansea login node, e.g
</p>
<pre>
  ssh user.name@login.HPCWales.co.uk
  ssh ssl001
</pre>
<p>
  To use subversion (svn), we need to setup a proxy for http/https. There
  is an http-proxy module, but subversion ignores the environment variable it sets.
  However, it can be used to see what the proxy should be. e.g
</p>
<pre>
>module show http-proxy
-------------------------------------------------------------------
/app/modules/system/http-proxy:

module-whatis	 set http proxy environment variables 
setenv		 http_proxy http://10.211.143.6:8080 
setenv		 https_proxy http://10.211.143.6:8080 
setenv		 ftp_proxy http://10.211.143.6:8080 
setenv		 all_proxy http://10.211.143.6:8080 
---------------------------------------------------------
</pre>
<p>
and then edit $HOME/subversion/servers to include    
</p>
<pre>
http-proxy-host = 10.211.143.6
http-proxy-port = 8080
</pre>

<p>
Although hdf5 etc are provided, there are only a few valid combinations of compiler, mpi, hdf5, python etc. We will assume GNU 4.8.0 with openmpi 1.8.5:
</p>
<pre>
module load subversion
module load compiler/gnu/4.8.0
module load mpi/openmpi/1.8.5
module load hdf5/1.8.13
module load python/2.7.9
module load petsc
</pre>
</div>
</body>
</html>

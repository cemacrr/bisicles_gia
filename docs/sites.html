<html>
<head>
  <title>BISICLES site specific notes</title>
</head>
<body>

<a href='index.html'>User documentation index<a/>

<h1>BISICLES site specific notes</h1>

<ol>
  <li><a href='#uob'>University of Bristol</a></li>
  <li><a href='#nersc'>NERSC</a></li>
  <li><a href='#archer'>ARCHER,</li>
  <li><a href='#monsoon'>Monsoon at the UK Met Office</a></li>
</ol>


<h2><a name='uob'>University of Bristol</a></h2>

<h3>Glaciology servers</h3>

<p>
The Bristol glaciology machines (dartagnan,athos,porthos) have a very old
gcc available by defualt (4.1.2), so you need to either build with the
<a href='oldbuild.html'>old instructions</a> or get access to newer compilers.
For the old way, just add
<pre>
module add openmpi/gnu_fc_gfortran
</pre>
to .bashrc. and the follow the <a href='oldbuild.html'>old instructions</a>.
For the new way, add
<pre>
GCCDIR=/data/ggslc/opt
export PATH=$GCCDIR/bin:$PATH
export LD_LIBRARY_PATH=$GCCDIR/lib64:$LD_LIBRARY_PATH
#also openmpi
OPENMPIDIR=/data/ggslc/opt/openmpi
export PATH=$OPENMPIDIR/bin:$PATH
export LD_LIBRARY_PATH=$OPENMPIDIR/lib:$LD_LIBRARY_PATH
</pre>
to .bashrc (at least till we can get CentOS upgraded) and 
<strong>remove the line that reads module add glacio-default-gnu</strong>
<p>

<p>
  porthos and athos do best with six processes (mpirun -np 6), dartagnan with 4 (mpirun -np 4)
</p>

<h3>Blue Crystal Phase 1, GNU compilers</h3>

<h3>Blue Crystal Phase 1, Portland Group compilers</h3>
To use the Portland Group (pgi,PG) compilers on Blue Crystal I, include
<pre>
  >module add pgi/64/11.10 openmpi/pgi/64/1.4.4
</pre>
in .bashrc. Compilers (for serial builds) are
</p>
<pre>
CC=pgcc
CXX=pgCC
FC=pgf90
</pre>
and for parallel builds the mpi wrappers are
<pre>
CC=mpicc
CXX=mpiCC
FC=mpif90
</pre>
Make.defs.local needs
<pre>
CXX           = pgCC
FC            = pgf90
XTRALDFLAGS   = -pgf90libs
MPICXX        = mpiCC
CPP           = $(CXX) -E
</pre>
Confirmed working with the MISMIP3D test on 1 and 2 nodes (4 and 8 processors).

<h3>Blue Crystal Phase 2</h3>
<p>
Blue Crystal II has, at least some of the time, slow access to networked
file systems. The major problem occurs with log output, such that when the
cluster is under heavy load BISICLES spends most of its time waiting to
write (a few bytes) to the pout.* files. The situation can be improved by
specifying that the pout.* files should be written to each node's local storage. 
Adding a line
</p>
<pre>
main.poutBaseName = /local/mydir/pout
</pre> 
<p>
would do the trick, provided that the directory mydir exists. Of course, you then
need to find a way to retrieve the pout.* files at the end of a run.
</p>
<p>
Creating the directories and retrieving the files can all be organized in the
submission script. Insert the lines
</p>
<pre>
for iii in `cat $PBS_NODEFILE | uniq` ; do 
  ssh $iii mkdir -p /local/$USER/
done
</pre>
<p>
somewhere in the script prior to the mpirun command. This will result in 
a directory /local/$USER being created on each node
that is being used.  To have the files moved to a network visible directory
$MYDIR, add 
</p>
<pre>
for iii in `cat $PBS_NODEFILE | uniq`;do 
   ssh $iii "cp /local/$USER/pout.* $MYDIR && rm /local/$USER/pout.*" 
done
</pre>
<p>
after the mpirun command. If the job is terminated before completion, the files
will remain on the nodes
</p>

<p>
We have used the GNU compilers extensively on Blue Crystal Phase 2, and have just started to test the Intel 12.0 compilers.
only runs with Intel...)
</p>

<h4>Blue Crystal Phase 2, GNU Compilers</h4>
To use the GNU compilers on Blue Crystal II, include
<pre>
  > module add ofed/openmpi/gcc/
  > module load languages/gcc-4.7
  > module load languages/python-2.7 #assuming you want the python interface
</pre>
in .bashrc. 
To build with the Python interface, $BISICLES_HOME/BISICLES/code/mk/Make.defs
needs the lines
<pre>
PYTHON_INC = usr/local/languages/python-2.7/include
PYTHON_LIBS = -lpython2.7
</pre>


<h4>Blue Crystal Phase 2, Intel Compilers</h4>
To use the Intel compilers on Blue Crystal II, include
<pre>
  > module load ofed/openmpi/intel
  > module load languages/python-2.7 #assuming you want the python interface
</pre>
in .bashrc. As of June 2013, ofed/openmpi/intel  loads two modules, languages/Intel-Composer-12 and ofed/openmpi/intel/64/1.4.2-qlc-testing
</p>
Compilers (for serial builds) are
</p>
<pre>
CC=icc
CXX=icpc
FC=ifort
</pre>
and for parallel builds the mpi wrappers are
<pre>
CC=mpicc
CXX=mpiCC
FC=mpif90
</pre>
Make.defs.local needs
<pre>
CXX           = icpc
FC            = ifort
MPICXX        = mpiCC
</pre>
To build with the Python interface, $BISICLES_HOME/BISICLES/code/mk/Make.defs
needs the lines
<pre>
PYTHON_INC = usr/local/languages/python-2.7/include
PYTHON_LIBS = -lpython2.7
</pre>
<p>
If we try to build in the usual way, we get an error
</p>
<pre>
../src/tarmac.f90(7): error #5149: Illegal character in statement label field  [m]
module glunnamed
...
../src/tarmac.f90(67): catastrophic error: Too many errors, exiting
</pre>
<p>
Which is caused by a flag (-FI) that is supplied to ifort by Chombo's build
system. For now , edit the file $BISICLES_HOME/Chombo/lib/mk/compiler/Make.defs.Intel
and change the lines
</p>
<pre>
  ifeq (0,$(shell test $(_ifcmajorver) -le 11 ; echo $$?))
    # -w95 gets rid of some annoying warnings about REAL*8, etc
    deffcomflags = -w95 -FI -u
  else
    deffcomflags = -FI -u
  endif
</pre>
<p>
to remove the '-FI' flags. 
</p>

<h3>Blue Crystal Phase 3</h3>

<h4>Blue Crystal Phase 3, Intel Compilers</h4>
<p>
To use the Intel compilers on Blue Crystal III, include
</p>
<pre>
module load intel-cluster-studio/impi/64
module load intel-cluster-studio/compiler/64
module load openmpi/intel/64
module load languages/python-2.7.5
</pre>
<p>
in .bashrc. To build with the Python interface, $BISICLES_HOME/BISICLES/code/mk/Make.defs
needs the lines
<pre>
PYTHON_INC=/cm/shared/languages/Python-2.7.5/include/python2.7/
PYTHON_LIBS=-L /cm/shared/languages/Python-2.7.5/lib/ -lpython2.7
</pre>
Apart from that, using the intel compilers on BC3 is the same as on BC2
</p>


<h2><a name='nersc'>NERSC</a></h2>
The US DOE-run National Energy Research Supercomputing Center ( <a href='http://www.nersc.gov'>NERSC</a>) has several machines which BISICLES users may find useful. 

<h3>Hopper -- GNU compilers</h3>
We have used <a href='http://www.nersc.gov/systems/hopper-cray-xe6/'> hopper.nersc.gov </a> extensively, particularly using the GNU compilers. BISICLES has been compiled against the PGI and CRAY compilers as well, but seems to be less stable; in particular, we have occasionally observed segfaults in PGI-compiled codes which do not occur when running identical problems with GNU builds.

<p> To build on Hopper, first load the appropriate modules. The following are the modules we currently use (as of August 24, 2012) in conjunction with BISICLES: 

<pre>
cornford@hopper11:~> module list                                                                                                            
Currently Loaded Modulefiles:                                                                                       
  1) modules/3.2.10.2                      14) ugni/5.0-1.0502.9685.4.24.gem                                        
  2) nsg/1.2.0                             15) pmi/5.0.6-1.0000.10439.140.3.gem                                     
  3) craype-network-gemini                 16) dmapp/7.0.1-1.0502.9501.5.211.gem                                    
  4) craype/2.2.1                          17) gni-headers/3.0-1.0502.9684.5.2.gem                                  
  5) eswrap/1.1.0-1.020200.1133.0          18) xpmem/0.1-2.0502.55507.3.2.gem                                       
  6) craype-mc12                           19) dvs/2.5_0.9.0-1.0502.1873.1.142.gem                                  
  7) cray-shmem/7.1.1                      20) alps/5.2.1-2.0502.9041.11.6.gem                                      
  8) cray-mpich/7.1.1                      21) rca/1.0.0-2.0502.53711.3.125.gem                                     
  9) torque/5.0.1                          22) atp/1.7.5                                                            
 10) moab/8.0.1-2014110616-5c7a394-sles11  23) PrgEnv-gnu/5.2.40                                                    
 11) gcc/4.9.2                             24) cray-hdf5-parallel/1.8.13                                            
 12) cray-libsci/13.0.1                    25) usg-default-modules/1.2                                              
 13) udreg/2.3.2-1.0502.9275.1.25.gem  
</pre>
which we get from:
<pre>
>module unload PrgEnv-pgi
>module load PrgEnv-gnu cray-hdf5-parallel
</pre>
Then, use the <pre>Make.defs.hopper</pre> file included in the Chombo release:

<pre>
cd Chombo/lib/mk/
ln -s local/Make.defs.hopper Make.defs.local
</pre>

(note that we recommend using symbolic links in this case to ensure that any changes to the 
released version of Make.defs.hopper (due to changes in hopper's configuration) are seamlessly included in your build process.

Finally, build the BISICLES driver:
<pre>
cd BISICLES/code/exec2D
gmake driver MPI=TRUE OPT=TRUE DEBUG=FALSE
</pre>

<h2><a name='archer'>ARCHER</a></h2>
<strong> In progress: for now are testing with  GNU and Intel, if that
does not work out or there is some demand we will try to fix cray</strong>

<p>
The Cray XC30 at ARCHER (UK National HPC) offers Cray, GNU, and Intel compilers.
Cray is the default (but doesn't seem to work for now). All three compilers can be supported with a single
Make.def.locals, it only only necessary to load the correct PrgEnv module and the cray-hdf5-parallel. 
Use the file <a href='Make.defs.archer'>Make.defs.archer</a> included in BISICLES/docs
(until we get it into the Chombo release)
</p>
<pre>
cp $BISICLES_HOME/BISICLES/docs/Make.defs.archer $BISICLES_HOME
ln -s $BISICLES_HOME/Make.defs.archer $BISICLES_HOME/Chombo/lib/mk/Make.defs.local
</pre>


<h3>GNU compilers </h3>
<p>
To compile with GNU, switch to the correct PrgEnv and load an hdf5 module
</p>
<pre>
> module swap PrgEnv-cray PrgEnv-gnu
> module load cray-hdf5-parallel 
</pre>

If you want the python interface, make sure that  $BISICLES_HOME/BISICLES/code/mk/Make.defs includes 
the following
<pre>           
ifneq ($(PYTHONHOME),)                                                                               
PYTHON_INC=$(PYTHONHOME)/include                                                                       
PYTHON_LIBS=-L$(PYTHONHOME)/lib -lpython2.7                                                            
endif 
</pre>
<p>
Then compile 
</p>
<pre>
cd BISICLES/code/exec2D
make all OPT=TRUE MPI=TRUE DEBUG=FALSE # are all default 
</pre>
<p>
the exectable will be driver2d.Linux.64.CC.ftn.OPT.MPI.GNU.ex
</p>

<p> ARCHER's compute nodes do not have access to the /home mount point (and therefore your home directory) 
It's not obvious where login scripts come from in that case, either. This can all be dealt with inside the submission script
(the MOM nodes do have access to home...). A minimal script (run from somewhere in /work) would look something like
<pre>
#PBS -l walltime=00:10:00 
#PBS -j oe 
#PBS -l select=1
#PBS -A /your allocation/  

export PBS_O_WORKDIR=$(readlink -f $PBS_O_WORKDIR)

#load modules needed to get GLIBCXX, HDF5, python where the compute nodes can see them
module swap PrgEnv-cray PrgEnv-gnu
module load cray-hdf5-parallel
module swap anaconda python-compute

cd $PBS_O_WORKDIR

EXE=/path/to/driver2d.Linux.64.CC.ftn.OPT.MPI.GNU.ex 
aprun -n 24 $EXE inputs.whatever
</pre>


<h3>Intel compilers </h3>

Compiling with Intel is essentially the same as with GNU. It's just a case of running
<pre>
> module swap PrgEnv-cray PrgEnv-intel
> module load cray-hdf5-parallel 
</pre>
instead of
<pre>
> module swap PrgEnv-cray PrgEnv-gnu
> module load cray-hdf5-parallel 
</pre>
The binary will be called driver2d.Linux.64.CC.ftn.OPT.MPI.INTEL.ex. Just as with GNU, 
the submission scripts needs to account for the fact that /home is not visible
on the compute nodes.


<h2><a name='monsoon'>Monsoon at the UK Met Office</a></h2>
<strong>Monsoon has been replaced, with a Cray of some sort. These instructions will be updated once we try out BISICLES
on the new machine. The instructions below are left just in case anyone trying to work with a similar system </strong>
<p>
Monsoon is an IBM AIX  system with the xl compilers installed. It is somewhat more
awkward to build with than the other clusters described: the xlf FORTRAN compiler doesn't distuingish between f77 and
f90 code  by the file extension, make is not GNU make (so use gmake instead) and
it is not possible to run mpi compiled jobs on the login nodes so that, for example, it is necessary to
create job submission scripts to configure parallel hdf5 (which is not installed on the system as of 17 Feb 2014)
</p>
<p>
ksh is the default shell. Whichever shell is used, 
it is necessary to somehow run 
<pre>
. prg_14_1_0_6
module load autotools/20131210
</pre>
Make.defs.local should contains:
<pre>
OPT           = TRUE
PRECISION     = DOUBLE
CXX           = xlC
FC            = xlf
MPI           = TRUE
MPICXX        = mpCC_r
USE_64        = TRUE
USE_HDF       = TRUE
#make sure BISICLES_HOME is correct
BISICLES_HOME=wherever/that/is
#make sure HDF_DIR is correct
HDFDIR=$(BISICLES_HOME)/hdf5/parallel
HDFINCFLAGS   = -I$(HDFDIR)/include -DH5_USE_16_API 
HDFLIBFLAGS   = -L$(HDFDIR)/lib -lhdf5 -lz
## Note: don't set the HDFMPI* variables if you don't have parallel HDF installed
HDFMPIINCFLAGS= -I$(HDFDIR)/include  -DH5_USE_16_API 
HDFMPILIBFLAGS= -L$(HDFDIR)/lib -lhdf5 -lz
syslibflags   = -L /critical/opt/lapack/3.4.0/lib/ -llapack_aix64_ -lblas_aix64_
</pre>

<p>
Attemping to configure and build hdf5 on the login node will fail, and a submission script is needed. 
</p>
<pre>
#!/bin/bash
#
# attempt to submit hdf5 parallel configure to a compute node.              
# This is necessary because jobs compiled with mpcc_r refuse
# to run on the login node...

#@ shell            = /usr/bin/bash
#@ class            = parallel
#@ job_type        = parallel
#@ job_name         = hdf5_parallel_configure
#@ output          = $(job_name).$(jobid).out
#@ error           = $(job_name).$(jobid).out
#@ node = 1
#@ tasks_per_node = 1
#@ notification     = error
#@ resources        = ConsumableCpus(1) ConsumableMemory(200mb)
#@ cpu_limit        = 00:10:00
#@ wall_clock_limit = 00:15:00
#@ queue

#This approved method doesn't seem to work...
#. prg_14_1_0_6
#module load autotools/20131210

PATH=/critical/opt/autotools/20131210/bin:/critical/opt/vacpp/v11.1.0.5/usr/vacpp/bin:/critical/opt/xlf/v14.1.0.6/usr/bin:/critical/opt/ukmo/mass/moose-monsoon-client-latest/bin:/opt/\
ibmhpc/pecurrent/ppe.poe/bin/:/usr/bin:/etc:/usr/sbin:/usr/bin/X11:/sbin:/usr/java5/jre/bin:/usr/java5/bin:/critical/opt/ukmo/supported/bin:/critical/opt/ukmo/freeware/bin:/opt/freewa\
re/bin

cd /home/slcorn/model/hdf5/parallel
CC=mpcc_r  AR="ar -X 64" ../hdf5-1.8.9/configure --prefix=/home/slcorn/model/hdf5/parallel --enable-shared=no
gmake
gmake install
exit 0
#End of script : submit with llsubmit 
</pre>


Steph also had to modify Chombo/lib/mk/compiler/Make.defs.IBM, Chombo/lib/mk/compiler/Make.rules:
xlf does not select free or fixed format code based on the file extension, and we have
relies on that feature up till now. There also seems to be something in the way that xlC 
handles class templates that meant modifications were needed to Chombo/lib/src/AMRElliptic/PetscSolver.H and PetscSolverI.H 
Diffs below, alternately contact steph (s.l.cornford@bristol.ac.uk) for the relevant files until 
either Chombo gets changed or we work something better out.
<pre>

bash-4.2$ svn diff ../../../Chombo/lib/mk/Make.rules            
Index: ../../../Chombo/lib/mk/Make.rules
===================================================================
--- ../../../Chombo/lib/mk/Make.rules   (revision 21511)
+++ ../../../Chombo/lib/mk/Make.rules   (working copy)
@@ -435,7 +435,7 @@
 
 
 o/$(config)/%.o : %.f90  d/$(config)/%.d
-       $(QUIET)$(FC) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) $< $(fobjflag)$@
+       $(QUIET)$(FC) $(F90FLAGS) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) $< $(fobjflag)$@
 
 
 o/$(config)/%.o : %.ChF  d/$(config)/%.d
@@ -446,15 +446,15 @@
 
 o/$(config)/%.o : %.F  d/$(config)/%.d
        $(QUIET)$(CSHELLCMD) "$(CH_CPP) $(CPPFLAGS) $(XTRACPPFLAGS) $(fcppflags) -DCH_LANG_FORT $< | $(fortpost) | awk 'NF>0' > f/$(config)/$*.f"
-       $(QUIET)$(FC) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) f/$(config)/$*.f $(fobjflag)$@
+       $(QUIET)$(FC) $(F77FLAGS) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) f/$(config)/$*.f $(fobjflag)$@
 
 o/$(config)/%.o : %.F90  d/$(config)/%.d
        $(QUIET)$(CSHELLCMD) "$(CPP) $(CPPFLAGS) $(XTRACPPFLAGS) $(fcppflags) -DCH_LANG_FORT $< | $(fortpost) | awk 'NF>0' > f/$(config)/$*.f90"
-       $(QUIET)$(FC) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) f/$(config)/$*.f90 $(fobjflag)$@
+       $(QUIET)$(FC) $(F90FLAGS) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) f/$(config)/$*.f90 $(fobjflag)$@
 
 
 o/$(config)/%.o : %.f  d/$(config)/%.d
-       $(QUIET)$(FC) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) $< $(fobjflag)$@
+       $(QUIET)$(FC) $(F77FLAGS) $(FFLAGS) $(XTRAFFLAGS) $(fcompflag) $< $(fobjflag)$@
 
 ##
 ## Rules to build ChomboFortran header files


bash-4.2$  svn diff ../../../Chombo/lib/mk/compiler/Make.defs.IBM
Index: ../../../Chombo/lib/mk/compiler/Make.defs.IBM
===================================================================
--- ../../../Chombo/lib/mk/compiler/Make.defs.IBM       (revision 21511)
+++ ../../../Chombo/lib/mk/compiler/Make.defs.IBM       (working copy)
@@ -50,8 +50,11 @@
     # -qtune will by default take on the value of -qarch (ndk)
     # -qstrict is strongly recommended to prevent the compiler from messing up the code <dbs>
     # -qsuppress suppresses messages: 1091 complains about friend declarations
-    # -qsuppress suppresses messages: 2907 complains about array bounds-checking    
-    defcxxcomflags = -qsuppress=1540-1091 -qsuppress=1540-2907 -qrtti=dynamiccast
+    # -qsuppress suppresses messages: 2907 complains about array bounds-checking
+    # 2704 complains about declarations     
+    F77FLAGS=-qfixed
+    F90FLAGS=-qfree
+    defcxxcomflags = -qsuppress=1540-1091 -qsuppress=1540-2907 -qsuppress=1540-0274 -qrtti=dynamiccast
     defcxxoptflags = -O3 -qstrict -qarch=auto # -qstaticinline -qinlglue -qlargepage
     defcxxdbgflags = -g -qflttrap -qfullpath -qdbxextra -qcheck=nullptr -qstatsym
 
@@ -73,14 +76,9 @@
     # xl* compile uses "-P" for output to file, so don't use it.
     # -C disables stripping "//" comments because they are valid Fortran code
     # -qsuppress suppresses messages: 234 complains about "0.0D0" float constants
-    ifeq ($(_xlcmajorver),8)
-      CH_CPP = $(CXX) -E -C -qnoppline -qsourcetype=c -qsuppress=1506-234
-    else ifeq ($(_xlcmajorver),10) # for bluefire.ucar.edu (DFM - 4/14/11)
-      CH_CPP = $(CXX) -E -C -qnoppline -qsourcetype=c -qsuppress=1506-234
-    else
-      CH_CPP = $(CXX) -E -C
-    endif
 
+      CH_CPP = $(CXX) -E -C -qnoppline -qsourcetype=c -qsuppress=1540-829
+
     # xlC appears to be OK with MT again...
     USE_MT = TRUE
   endif
@@ -102,7 +100,7 @@
     # -qstrict makes sure the compiler optimizer doesn't change the results <dbs>
     # -qsuppress suppresses messages: 510 is the "compilation successful" msg
     deffcomflags = -qsuppress=1501-510
-    deffoptflags = -O3 -qarch=auto -qfixed -qmaxmem=99999 -qhot -qstrict
+    deffoptflags = -O3 -qarch=auto  -qmaxmem=99999 -qhot -qstrict
     deffdbgflags = -g -C -qfullpath # -qextchk
     deffprofflags = -pg
     ifneq ($(USE_EXTNAME),FALSE)


bash-4.2$  svn diff ../../../Chombo/lib/src/AMRElliptic/PetscSolver.H
Index: ../../../Chombo/lib/src/AMRElliptic/PetscSolver.H
===================================================================
--- ../../../Chombo/lib/src/AMRElliptic/PetscSolver.H   (revision 21511)
+++ ../../../Chombo/lib/src/AMRElliptic/PetscSolver.H   (working copy)
@@ -269,10 +269,10 @@
 
 #include "NamespaceFooter.H"
 
-#ifdef CH_USE_PETSC
+//#ifdef CH_USE_PETSC
 #ifndef CH_EXPLICIT_TEMPLATES
 #include "PetscSolverI.H"
 #endif // CH_EXPLICIT_TEMPLATES
-#endif
+//#endif
 
 #endif /*_PETSCSOLVER_H_*/

../../Chombo/lib/src/AMRElliptic/PetscSolverI.H
===================================================================
--- ../../../Chombo/lib/src/AMRElliptic/PetscSolverI.H  (revision 21511)
+++ ../../../Chombo/lib/src/AMRElliptic/PetscSolverI.H  (working copy)
@@ -18,14 +18,17 @@
 #include "memusage.H"
 #include "NamespaceHeader.H"
 
+#ifdef CH_USE_PETSC
 #include <private/kspimpl.h>   /*I "petscksp.h" I*/
 #include <private/pcimpl.h>
+#endif
 
 // *******************************************************
 // PetscSolver Implementation
 // *******************************************************
 template <class T>
 PetscSolver<T>::PetscSolver()
+#ifdef CH_USE_PETSC
   :m_homogeneous(false),
    m_mat(0), // m_xx, m_rr, m_bb;
    m_ksp(0),
@@ -36,6 +39,7 @@
    m_null(false),
    m_nz_init_guess(false),
    m_gid0(0)
+#endif
 {
   m_dx = 0.;
 }
@@ -43,16 +47,18 @@
 template <class T>
 void PetscSolver<T>::destroy()
 {
+#ifdef CH_USE_PETSC
   if ( m_defined )
   {
-#ifdef CH_USE_PETSC
+
     MatDestroy(&m_mat);
     VecDestroy(&m_bb);
     VecDestroy(&m_xx);
     VecDestroy(&m_rr);
-#endif
     m_defined = 0;
+
   }
+#endif
 #ifdef CH_USE_PETSC
   if ( m_ksp )
     {

</pre>

</body>
</html>

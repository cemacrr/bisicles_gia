<html>
<head>
  <title>BISICLES build instructions</title>
  <link href="bikedoc.css" rel="stylesheet" type="text/css"/>
</head>
<body>

<div id="head">
<ul>
<li><a href='index.html'>Index page<a/> </li>
<li><a href='#top'>Top of page<a/> </li>
</ul>

<h1>Contents</h1>

<ol>
  <li> <a href='#sysreq'>System requirements</a></li>
  <li> <a href='#svnco'>Source code</a></li>
  <li> <a href='#deps'>Dependencies</a></li>
  <li> <a href='#chombo'>Chombo configuration</a></li>
  <li> <a href='#bikeconf'>BISICLES configuration</a></li>
  <li> <a href='#bisicles'>Building basic BISICLES</a></li>
  <li> <a href='#example'>MISMIP3d example</a></li>
  <li> <a href='#all'>Building all of BISICLES</a></li>
</ol>

</div>

<div id="main">
<h1><a name='top'>BISICLES build instructions</a></h1>

<p>These are the instructions to build BISICLES against the Chombo 3.2 release. To build against the older 3.1 Chombo release, go <a href='readme-3.1.html'>here<a/>

<p>To build (and run) BISICLES you need to</p>
<ol>
  <li> Meet the <a href='#sysreq'>system requirements</a></li>
  <li> <a href='#svnco'>Check out</a> the source code</li>
  <li> Set up some third party <a href='#deps'>dependencies</a></li>
  <li> <a href='#chombo'> Configure Chombo</a> by editing some definitions in a makefile</li>
  <li> <a href='#bikeconf'>Configure BISICLES</a> addtional options by editing some definitions in another makefile</li>
  <li> <a href='#bisicles'> Compile driver</a>, the main standalone BISICLES executable.</li>
  <li> <a href='#example'> Run driver on a simple problem </a> to ensure that it works</li>
</ol>


<h2><a name="sysreq"> System requirements.</a></h2>
<p> 
  BISICLES requires the GNU/Linux operating system (actually, it should compile and run elsewhere,
  but we never do that), plus both C++ and FORTRAN compilers, a GNU-compatible make, and Perl.
  On the whole, life is easiest with gcc (including g++ and gfortran), and we shall assume that
  is what will be used. A modernish gcc is required - see the <a href='oldbuild.html'>older build instructions<a> if
   you want to build with gcc 4.1.2, for example. To build the parallel version, you need an MPI environment, which provides 
  the mpicc and mpicxx  (or equivalents). You will also need 
  <a href='https://wci.llnl.gov/codes/visit/home.html'>VisIt</a></li> to view the data BISICLES 
produces (other programs can be put to use, but VisIt is by far the most convenient)</p>
<p>
  There are some <a href='sites.html'>site specific notes</a> 
</p>
<h2><a name="svnco"> Check out the source code.</a> </h2>
<p> Since this readme file lives in the source code repository, you might have already checked the source code
  out. There are two source trees, Chombo, and BISICLES, and the rest of this guide assumes that you
  have a directory called $BISICLES_HOME which contains the two.</p>

<p> To obtain the source trees, you first need an <a href='https://anag-repo.lbl.gov/'>ANAG repository account</a>.
  Once that is sorted out, create a root directory for both source trees
<pre>
  > export BISICLES_HOME=/wherever/you/like #assumes bash...
  > mkdir -p $BISICLES_HOME
  > cd $BISICLES_HOME
  > svn co https://anag-repo.lbl.gov/svn/Chombo/release/3.2 Chombo 
  > svn co https://anag-repo.lbl.gov/svn/BISICLES/public/trunk BISICLES 
</pre>
</p> 

<h2><a name="deps"> Dependencies.</a> </h2>
<p>
  If you're working on a system which is maintained by somebody else (like, for example, the supercomputers at NERSC), it's likely that most, if not all, of these dependencies have been built and installed already, which can save you some effort. So, check to see what's already installed, with an eye open to the possibility that things have been configured in some odd way which makes them unusable for us, in which case you you're back to where you were anyway)

  Chombo requires the hdf5 libraries. As of the Chombo 3.2 release, Chombo builds against the current hdf5 1.8 interface directly. (Chombo 3.1 and earlier built against the older hdf5 1.6 interface, which required some effort when building)

</p>
Chombo 3.2 also contains an optional interface to the <a href='http://www.mcs.anl.gov/petsc/'> PETSc</a>  solver library. We have found that using the Chombo AMR interface to the petsc algebraic multigrid solvers (either GAMG or Hypre's BoomerAMG) can substantially improve the performance of the nonlinear ice velocity solve for problems where the native Chombo geometric multigrid (GMG) solvers struggle. 

</p>
<p>  Glimmer-CISM also requires netcdf, and in a non-standard configuration to boot. BISICLES
  includes some complementary tools which require netcdf. So, while you could use the
  version installed on your system 
  it is often simpler to compile netCDF from source.
</p>
<p>
  There should be a script, download_dependencies.sh (in the same directrory as this file, 
  $BISICLES_HOME/BISICLES/docs) that will get the (version 1.8.9) hdf5 sources and  
  unpack them, twice : once into hdf5/serial/src and once into hdf5/parallel/src. 
  It assumes $BISICLES_HOME is set. It should contain the following
<pre>
cd $BISICLES_HOME
echo `pwd`



#get hdf5 sources
if !(test -e hdf5-1.8.9.tar.bz2) then
    echo "downloading hdf5-1.8.10.tar.gz"
    wget http://www.hdfgroup.org/ftp/HDF5/prev-releases/hdf5-1.8.9/src/hdf5-1.8.9.tar.bz2
fi
mkdir -p hdf5/parallel/src
tar -jxf  hdf5-1.8.9.tar.bz2 -C hdf5/parallel/src

mkdir -p hdf5/serial/src
tar -jxf  hdf5-1.8.9.tar.bz2 -C hdf5/serial/src


#get netcdf sources

if !(test -e netcdf-4.1.3.tar.gz) then
    echo "downloading netcdf-4.1.3.tar.gz"
    wget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-4.1.3.tar.gz
fi
mkdir -p netcdf/parallel/src
tar -zxf netcdf-4.1.3.tar.gz -C netcdf/parallel/src

mkdir -p netcdf/serial/src
tar -zxf netcdf-4.1.3.tar.gz -C netcdf/serial/src
</pre>


  If you want to build a single-processor BISICLES, then build hdf5 in hdf5/serial/src, 
  and if you want to build a multi-processor BISICLES, then build hdf5 in hdf5/parallel/src.
  Similar pairs of directories will be built for netcdf.
</p>
<h3>Building serial hdf5 </h3>
Before starting, make sure that you can run gcc.
Enter the appropriate source directory
<pre>
> cd $BISICLES_HOME/hdf5/serial/src/hdf5-1.8.9/
</pre>
and configure hdf5 like so
<pre>
CC=gcc CFLAGS=-fPIC ./configure --prefix=$BISICLES_HOME/hdf5/serial/ --enable-shared=no
</pre>
note the --enable-shared=no. This isn't strictly necessary - you could use shared libraries -
but we find they are more trouble than they are worth, especially when running on clusters.
The -fPIC flag will be useful later if you want to build the experimental <a href='libamrfile.html'>libamrfile</a>
shared library that can be used to manipulate Chombo (and BISICLES) output with languages 
that support a plain C function calling convention, like FORTRAN 90, GNU R, Python and MATLAB. 
Configure will spit out a long list of tests, and hopefully pass the all to produce a report
that reads as follows:
<pre>

            SUMMARY OF THE HDF5 CONFIGURATION
            =================================

...

Languages:
----------
                        Fortran: no

                            C++: no

Features:
---------
                  Parallel HDF5: no
             High Level library: yes
                   Threadsafety: no
            Default API Mapping: v18
 With Deprecated Public Symbols: yes
         I/O filters (external): deflate(zlib)
         I/O filters (internal): shuffle,fletcher32,nbit,scaleoffset
                            MPE: no
                     Direct VFD: no
                        dmalloc: no
Clear file buffers before write: yes
           Using memory checker: no
         Function Stack Tracing: no
                           GPFS: no
      Strict File Format Checks: no
   Optimization Instrumentation: no
       Large File Support (LFS): yes


</pre>
Don't worry that the C++ and Fortran languages are not enabled : Chombo uses the C interface, 
and (when we come to compile that too), so does netcdf. Assuming this is all OK, type
<pre>
> make install
</pre>
and you, after a round of compiling and copying, you 
should see that the hdf5 libraries bin,doc,include and src have appeared in $BISICLES_HOME/hdf5/serial/.
</p>
<h3>Building parallel hdf5 </h3>

Before starting, make sure that the mpi environment is in place and that you can run mpicc.
Enter hdf5/parallel/src/hdf5-1.6.10/ directory
<pre>
> cd $BISICLES_HOME/hdf5/parallel/src/hdf5-1.8.9/
</pre>
and configure hdf5, this time enabling MPI through the use of mpicc in place of gcc
<pre>
> CC=mpicc ./configure --prefix=$BISICLES_HOME/hdf5/parallel/ --enable-shared=no
</pre>
This time, configure's final report should read
<pre>

            SUMMARY OF THE HDF5 CONFIGURATION
            =================================
(...)

Languages:
----------
                        Fortran: no

                            C++: no

Features:
---------
                  Parallel HDF5: mpicc
             High Level library: yes
                   Threadsafety: no
            Default API Mapping: v18
 With Deprecated Public Symbols: yes
         I/O filters (external): deflate(zlib)
         I/O filters (internal): shuffle,fletcher32,nbit,scaleoffset
                            MPE: 
                     Direct VFD: no
                        dmalloc: no
Clear file buffers before write: yes
           Using memory checker: no
         Function Stack Tracing: no
                           GPFS: no
      Strict File Format Checks: no
   Optimization Instrumentation: no
       Large File Support (LFS): yes
</pre>
Note especially the line 'Parallel HDF5: mpicc '
</body>
Assuming this is all OK, type
<pre>
> make install
</pre>
this time, the bin,doc,include and src directories should end up  in $BISICLES_HOME/hdf5/parallel/.
</p>

<h3><a name='cdf'>Building serial netcdf</a> </h3>
<p>
Before starting, make sure that you can run gcc,g++ and gfortran.
Enter the appropriate source directory
<pre>
> cd $BISICLES_HOME/netcdf/serial/src/netcdf-4.1.3/
</pre>
Now, netcdf needs to link hdf5 : it doesn't really matter which version but we might as
well use the one we have. So, we have 
a custom configure line
<pre>
> CC=gcc CPPFLAGS=-I$BISICLES_HOME/hdf5/serial/include/ CXX=g++ FC=gfortran LDFLAGS=-L$BISICLES_HOME/hdf5/serial/lib/  ./configure --prefix=$BISICLES_HOME/netcdf/serial   --enable-shared=no --enable-static=yes --enable-dap=no
</pre>
Next, compile, test, and install netcdf
<pre>
> make check install
</pre>
and assuming all goes well, netcdf 4.1.3 will now be installed in $BISICLES_HOME/netcdf/serial
</p>
<h3>Building parallel netcdf </h3>

<p>
So far, the only difference between parallel and serial netcdf installs 
is the link to parallel hdf5 and the use of the MPI compiler wrapper. 
Possibly, building two versions of netcdf is a waste of time. 
</p>
<p><strong>update</strong> parallel netcdf may be needed to compile parallel glimmer-CISM, otherwise it can be skipped.</p>
<pre>
> cd $BISICLES_HOME/netcdf/parallel/src/netcdf-4.1.3/
> CC=mpicc CXX=mpiCC FC=mpif90  CPPFLAGS="-DgFortran -I$BISICLES_HOME/hdf5/parallel/include/" LDFLAGS=-L$BISICLES_HOME/hdf5/parallel/lib/  ./configure --prefix=$BISICLES_HOME/netcdf/parallel  --enable-shared=no --enable-static=yes --enable-dap=no
> make  check install
</pre>

</p>
<h3><a name="petsc">Installing PETSc</a></h3>
<p>
If we're planning to use the PETSc solver interface, it's a good idea to install PETSc before building Chombo. It's likely that some version of petsc may be pre-installed on your system -- we need petsc version 3.3.4 or later,

<ol>
<li> First, download PETSc:
<pre>
git clone -b maint https://bitbucket.org/petsc/petsc petsc
</pre>

<li>  Configure petsc. This is the configure command that I used (this builds a parallel version and installs it in /usr/local/petsc/petsc-maint-mpi):
<pre>
cd petsc
./configure --download-fblaslapack=yes --download-hypre=yes -with-x=0 --with-c++support=yes --with-mpi=yes --with-hypre=yes --prefix=/usr/local/petsc/petsc-maint-mpi --with-c2html=0 --with-ssl=0
</pre> 


The petsc install system is pretty helpful and will tell you what to do
if it runs into problems (unlike, say, Chombo)

<li> Follow the instructions to make and install the library

<li> Finally, set the PETSC_DIR and PETSC_ARCH environment variables, which Chombo will use in order to find your PETSc installation. For my installation (I'm using tcsh) 
<pre>
setenv PETSC_DIR /usr/local/petsc/petsc-maint-mpi
setenv PETSC_ARCH arch-linux2-c-debug
</pre>
if using bash 
<pre>
export PETSC_DIR=/usr/local/petsc/petsc-maint-mpi
export PETSC_ARCH=arch-linux2-c-debug
</pre>
</ol>


</p>
<h2><a name="chombo">Chombo configuration</a></h2>
<p>
Next we need to set up Combo's configuration (which BISICLES will inherit automatically).
The main task here is create a file called $BISICLES_HOME/Chombo/lib/mk/Make.defs.local,
and there is version stored in this directory that should be easy enough to edit.
First, copy it into $BISICLES_HOME
<pre>
> cp $BISICLES_HOME/BISICLES/docs/Make.defs.local $BISICLES_HOME
</pre>
At the very least, you will need to  edit the line that reads
<pre> 
BISICLES_HOME=..., 
</pre>
to give the correct value. If you don't have MPI, there are a few lines to comment out. 
You might also want to tinker with the optimization flags and so on. Then create a link
so that Chombo sees Make.defs.local in the place it expects
<pre>
>ln -s $BISICLES_HOME/Make.defs.local $BISICLES_HOME/Chombo/lib/mk/Make.defs.local
</pre>

<h2><a name="bikeconf">Configuring BISICLES</a></h2>

<p>
A makefile containing options specific to BISICLES (rather than Chombo) 
is located at
</p>
<pre>
$BISICLES_HOME/BISICLES/code/mk/Make.defs
</pre>
<p>
You do not usually need to edit that file, but instead, add a file
named for your machine to that directory. Run 'uname -n' to find out the 
name of you machine, e.g on a host called 'mymachine'
</p>
<pre>
> cd $BISICLES_HOME/BISICLES/code/mk/
> uname -n
mymachine
> cp Make.defs.template Make.defs.mymachine
</pre>
<p>
Alternatively, create a file called Make.defs.none
</p>



<h3>Python</h3>
<p>
To make use of the <a href='pythoninterface.html'>python interface</a>, you need to 
ensure that you have a suitable
python installation. This is usually straightforward in modern GNU/linux distributions,
since Python is so widespread. The aim is to make sure that the
the variables PYTHON_INC and PYTHON_LIBS are correctly defined. 
Make.defs.template attempts to set these variables by running
</p>
<pre>
python-config --includes
python-config --libs
</pre>
which works on many workstations but may not be what you want.
In that case, you need to find out where the header file "Python.h" lives,
and what linker flags you need. For example, edit  Make.defs.mymachine to set
<pre>
PYTHON_INC=/usr/include/python2.7
PYTHON_LIBS=-lpython2.7
</pre> 
<p>
There are several machine specific examples in the same directory. If you do not want
the python interface for some reason (we advise having it), make sure that
PYTHON_INC is not set</p>

<h3>NetCDF</h3>
<p>
Netcdf is not needed by the main BISICLES code, but there are
tools and examples to do depend upon it. The aim is to set the
NETCDF_INC and NETCDF_LIBS variables in (e.g) Make.defs.mymachine, and
once again Make.defs.template shows one way to do this if 
netcdf is installed, by running
</p>
<pre>
nc-config --includedir
nc-config --flibs
</pre>
<p>
If that is not suitable, for example if you built <a href='#cdf'>netcdf as above</a>,  
you need to find the location netcdf.h and netcdf.mod
and the correct linker flags (for fortran and C), e.g edit  Make.defs.mymachine to set
</p>
<pre>
NETCDF_INC=$(BISICLES_HOME)/netcdf/serial/include
NETCDF_LIBS=-L$(BISICLES_HOME)/netcdf/serial/lib -lnetcdff -lnetcdf -lhdf5_hl
</pre>

<h2><a name="bisicles">Building basic (standalone) BISICLES</a></h2>

<p>Now we are ready to build one or more BISICLES executables. If you plan to do development
work on the code itself, you will want to build an unoptimized version to run in gdb. Run
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all
</pre>
This will build a set of Chombo libraries, and then BISICLES. Hopefully, it will complete without errors, and you
will end up with an executable called <quote>driver2d.Linux.64.g++.gfortran.DEBUG.ex</quote>. 
This one is most useful for low-level debugging of the code - if you 
are not planning to do that, there is no need for it.  If you have a serial computer only, run
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all OPT=TRUE
</pre>
to get an optimized executable called <quote>driver2d.Linux.64.g++.gfortran.DEBUG.OPT.ex</quote>. 
</p>
An optimized  parallel executable <quote>driver2d.Linux.64.mpic++.gfortran.DEBUG.OPT.MPI.ex</quote> can be built like so
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all OPT=TRUE MPI=TRUE
</pre>
For serious runs, this is the one you need. Even on workstations with a few processors (like dartagnan.ggy.bris.ac.uk) noticeable (2X-4X)  speed improvements are realized by running parallel code, and on clusters like bluecrystal or hopper.nersc.gov we have obtained 100X speedups for big enough problems (and hope to obtain 1000X speedups).
<p>
<p>
Finally, should you feel the urge, you can have a non-optimized parallel version, which can be used for hunting down low-level
bugs that crop up in parallel operation but not in serial operation. 
</p>
To build with PETSc support, add "USE_PETSC=TRUE" to your build line, e.g.
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all OPT=TRUE MPI=TRUE USE_PETSC=TRUE
</pre>

<h3><a name="makeclean">Make clean</a></h3>
<p>On occasion it might be necessary to rebuild BISICLES entirely, rather than just those parts where a file
has changed. To do so, run
<pre>make clean</pre>
for unoptimized, serial builds, 
<pre>make clean OPT=TRUE</pre>
for optimized serial builds, and
<pre>make clean OPT=TRUE MPI=TRUE</pre>
for optimized parallel builds</p>

<h3><a name="makerealclean">Make realclean</a></h3>
<p> In the event even more houscleaning is desired, the "realclean" target does everything the "clean" target does, and additionally removes many other user-generated files, including all files with the ".hdf5" suffix (including checkpoint and plot files). 

<h2><a name="example">Running BISICLES on a simple problem</a></h2>
<p>
All the data to run <a href='http://homepages.ulb.ac.be/~fpattyn/mismip3d/welcome.html'>Frank Pattyn's MISMIP3D P075 experiment</a> is already
present. Change to the MISMIP3D subdirectory, and generate some input files from a template.
<pre>
> cd $BISICLES_HOME/BISICLES/examples/MISMIP3D
> sh make_inputs.sh
</pre>
then we are ready to go.
</p>
<h3>Running on a serial Workstation</h3>
On a serial machine, try the 3 AMR level problem (which will have
a resolution of 800 m at the grounding line, and 6.25 km far from it).
<pre>
$BISICLES_HOME/BISICLES/code/exec2D/driver2d.Linux.64.g++.gfortran.DEBUG.OPT.ex  inputs.mismip3D.p075.l1l2.l3 > sout.0 2>err.0 &
</pre>
You can watch progress by typing
<pre>
> tail -f sout.0
</pre>
and eventually, you will get a series of plot*2d.hdf5 files that you can view in visit
</p>

<h3>Running on a parallel Workstation</h3>
If you have a parallel machine, run the 5 AMR level problem (which will have
a resolution of 200 m at the grounding line, and 6.25 km far from it).
<pre>
> nohup mpirun -np 4 $BISICLES_HOME/BISICLES/code/exec2D/driver2d.Linux.64.mpic++.gfortran.DEBUG.OPT.MPI.ex inputs.mismip3D.p075.l1l2.l5 &
</pre>
replace -np 4 with the appropriate count for your machine: this is usually <strong>not</strong> the number
of CPU cores because these will typically share some resources. A good guess is 
the number of memory buses. Ideally, do a series of scaling experiments
to come up with a graph like this one 
</p>
<img src="porthos.png" alt='scaling with CPU count on porthos.ggy.bris.ac.uk, 5 level MISMIP3D problem'/>
<p>
See also the <a href='sites.html'>Site specific notes</a>
</p>

<p>
You can watch progress by typing
<pre>
> tail -f pout.0
</pre>
and eventually, you will get a series of plot*2d.hdf5 files that you can view in visit
</p>
<h3>Running on a cluster</h3>

See the <a href='sites.html'>Site specific notes</a>

<h3> Using the PETSc solvers</h3> 
The examples presented in this tutorial all use the default Chombo geometric multigrid (GMG) solvers. If you find that convergence of the velocity solver is stalling (or even diverging), you might want to try a PETSc algebraic multigrid (AMG) solver. To switch to the PETSc solvers,
<ol>
<li> You will need a <pre>.petsrc</pre> file to tell the PETSc solvers what to do. There is an example one in the exec2D directory which is set to parameters which have worked well for us. For example, it uses the <a href='http://computation.llnl.gov/casc/linear_solvers/sls_hypre.html'>HYPRE </a> BoomerAMG solver. 

<li> Edit your BISICLES input file. There are two ways to use PETSC:
<ul>
<li> As a bottom solver in the Chombo GMG solver. (if you don't know what that means, don't worry -- suffice it to say that in many cases, this option occupies the middle ground between the Chombo native GMG solvers and a full AMR AMG solver.). Assuming you are using the JFNKSolver for your velocity solve, add the following line to your inputs file:
<pre>
JFNKSolver.bottom_solver_type = 1
</pre>

<li> If that doesn't help, then you're ready for the full AMR PETSc solver. Assuming once again that you're using the JFNKSolver, then change 
<pre> 
JFNKSolver.solverType = 0
</pre>
to 
<pre> 
JFNKSolver.solverType = 4
<\pre>

<\ul>

</ol>



<h2><a name='all'>Building all of BISICLES</a></h2>

<p>
Once BISICLES is working on a given machine, it might be convenient to
build everything -  standalone BISICLES, 
<a href='libamrfile.html'>the R/Python/MATLAB analysis tools</a></li>, 
the programmable <a href='cdriver.html'>cdriver interface</a>, the
<a href='filetools.html'>file tools</a>, and so on. To build
everything, run e.g
</p>
<pre>
cd $BISICLES_HOME/BISICLES/code
make all OPT=TRUE MPI=TRUE
</pre>
<p>
or specify the optons you prefer. 
</p>
</div>

</body>
</html>

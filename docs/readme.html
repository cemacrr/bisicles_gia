<html>
<head>
  <title>BISICLES build instructions</title>
</head>
<body>

<a href='index.html'>User documentation index<a/>

<h1>BISICLES build instructions</h1>

<p>These are the instructions to build BISICLES against the Chombo 3.2 release. To build against the older 3.1 Chombo release, go <a href='readme-3.1.html'>here<a/>

<p>To build (and run) BISICLES you need to</p>
<ol>
  <li> Meet the <a href='#sysreq'>system requirements</a></li>
  <li> <a href='#svnco'>Check out</a> the source code</li>
  <li> Set up some third party <a href='#deps'>dependencies</a></li>
  <li> <a href='#chombo'> Configure Chombo</a> by editing some definitions in a makefile</li>
  <li> <a href='#python'> Configure the (optional) python interface</a> by editing some definitions in a makefile</li>
  <li> <a href='#bisicles'> Compile driver</a>, the main standalone BISICLES executable.</li>
  <li> <a href='#example'> Run driver on a simple problem </a> to ensure that it works</li>
</ol>
<p> Optionally, you might also</p>
<ol>
  <!--<li> visualize BISICLES output using <a href='https://wci.llnl.gov/codes/visit/home.html'>VisIt</a></li>!-->
  <li> <a href='#nctoamr'> Compile the filetools</a>, including nctoamr, a tool to convert NetCDF files into hdf5 files that BISICLES can read.</li>
  <li> <a href='#control'> Compile control</a>, which can be used to solve an inverse problem (or control problem)</li>
  <!--<li> <li> Look at an example application, <a href='pineisland.html'>Pine Island Glacier</a>, to 
  see how to load topography, thickness, basal traction coefficient, and temperature</li>
  <li> Specify <a href='surfaceflux.html'>surface thickness fluxes</a>, that is, accumulation and melt rates</li>!-->
</ol>



<h2><a name="sysreq"> System requirements.</a></h2>
<p> 
  BISICLES requires the GNU/Linux operating system (actually, it should compile and run elsewhere,
  but we never do that), plus both C++ and FORTRAN compilers, a GNU-compatible make, and Perl.
  On the whole, life is easiest with gcc (including g++ and gfortran), and we shall assume that
  is what will be used. A modernish gcc is required - see the <a href='oldbuild.html'>older build instructions<a> if
   you want to build with gcc 4.1.2, for example. To build the parallel version, you need an MPI environment, which provides 
  the mpicc and mpicxx  (or equivalents). You will also need 
  <a href='https://wci.llnl.gov/codes/visit/home.html'>VisIt</a></li> to view the data BISICLES 
produces (other programs can be put to use, but VisIt is by far the most convenient)</p>
<p>
  There are some <a href='sites.html'>site specific notes</a> 
</p>
<h2><a name="svnco"> Check out the source code.</a> </h2>
<p> Since this readme file lives in the source code repository, you might have already checked the source code
  out. There are two source trees, Chombo, and BISICLES, and the rest of this guide assumes that you
  have a directory called $BISICLES_HOME which contains the two.</p>

<p> To obtain the source trees, you first need an <a href='https://anag-repo.lbl.gov/'>ANAG repository account</a>.
  Once that is sorted out, create a root directory for both source trees
<pre>
  > export BISICLES_HOME=/wherever/you/like #assumes bash...
  > mkdir -p $BISICLES_HOME
  > cd $BISICLES_HOME
  > svn co https://anag-repo.lbl.gov/svn/Chombo/release/3.2 Chombo 
  > svn co https://anag-repo.lbl.gov/svn/BISICLES/public/trunk BISICLES 
</pre>
</p> 

<h2><a name="deps"> Dependencies.</a> </h2>
<p>
  If you're working on a system which is maintained by somebody else (like, for example, the supercomputers at NERSC), it's likely that most, if not all, of these dependencies have been built and installed already, which can save you some effort. So, check to see what's already installed, with an eye open to the possibility that things have been configured in some odd way which makes them unusable for us, in which case you you're back to where you were anyway)

  Chombo requires the hdf5 libraries. As of the Chombo 3.2 release, Chombo builds against the current hdf5 1.8 interface directly. (Chombo 3.1 and earlier built against the older hdf5 1.6 interface, which required some effort when building)

</p>
Chombo 3.2 also contains an optional interface to the <a href='http://www.mcs.anl.gov/petsc/'> PETSc</a>  solver library. We have found that using the Chombo AMR interface to the petsc algebraic multigrid solvers (either GAMG or Hypre's BoomerAMG) can substantially improve the performance of the nonlinear ice velocity solve for problems where the native Chombo geometric multigrid (GMG) solvers struggle. 

</p>
<p>  Glimmer-CISM also requires netcdf, and in a non-standard configuration to boot. BISICLES
  includes some complementary tools which require netcdf. So, while you could use the
  version installed on your system 
  it is often simpler to compile netCDF from source.
</p>
<p>
  There should be a script, download_dependencies.sh (in the same directrory as this file, 
  $BISICLES_HOME/BISICLES/docs) that will get the (version 1.8.9) hdf5 sources and  
  unpack them, twice : once into hdf5/serial/src and once into hdf5/parallel/src. 
  It assumes $BISICLES_HOME is set. It should contain the following
<pre>
cd $BISICLES_HOME
echo `pwd`



#get hdf5 sources
if !(test -e hdf5-1.8.9.tar.bz2) then
    echo "downloading hdf5-1.8.10.tar.gz"
    wget http://www.hdfgroup.org/ftp/HDF5/prev-releases/hdf5-1.8.9/src/hdf5-1.8.9.tar.bz2
fi
mkdir -p hdf5/parallel/src
tar -jxf  hdf5-1.8.9.tar.bz2 -C hdf5/parallel/src

mkdir -p hdf5/serial/src
tar -jxf  hdf5-1.8.9.tar.bz2 -C hdf5/serial/src


#get netcdf sources

if !(test -e netcdf-4.1.3.tar.gz) then
    echo "downloading netcdf-4.1.3.tar.gz"
    wget ftp://ftp.unidata.ucar.edu/pub/netcdf/netcdf-4.1.3.tar.gz
fi
mkdir -p netcdf/parallel/src
tar -zxf netcdf-4.1.3.tar.gz -C netcdf/parallel/src

mkdir -p netcdf/serial/src
tar -zxf netcdf-4.1.3.tar.gz -C netcdf/serial/src
</pre>


  If you want to build a single-processor BISICLES, then build hdf5 in hdf5/serial/src, 
  and if you want to build a multi-processor BISICLES, then build hdf5 in hdf5/parallel/src.
  Similar pairs of directories will be built for netcdf.
</p>
<h3>Building serial hdf5 </h3>
Before starting, make sure that you can run gcc.
Enter the appropriate source directory
<pre>
> cd $BISICLES_HOME/hdf5/serial/src/hdf5-1.8.9/
</pre>
and configure hdf5 like so
<pre>
CC=gcc CFLAGS=-fPIC ./configure --prefix=$BISICLES_HOME/hdf5/serial/ --enable-shared=no
</pre>
note the --enable-shared=no. This isn't strictly necessary - you could use shared libraries -
but we find they are more trouble than they are worth, especially when running on clusters.
The -fPIC flag will be useful later if you want to build the experimental <a href='libamrfile.html'>libamrfile</a>
shared library that can be used to manipulate Chombo (and BISICLES) output with languages 
that support a plain C function calling convention, like FORTRAN 90, GNU R, Python and MATLAB. 
Configure will spit out a long list of tests, and hopefully pass the all to produce a report
that reads as follows:
<pre>

            SUMMARY OF THE HDF5 CONFIGURATION
            =================================

...

Languages:
----------
                        Fortran: no

                            C++: no

Features:
---------
                  Parallel HDF5: no
             High Level library: yes
                   Threadsafety: no
            Default API Mapping: v18
 With Deprecated Public Symbols: yes
         I/O filters (external): deflate(zlib)
         I/O filters (internal): shuffle,fletcher32,nbit,scaleoffset
                            MPE: no
                     Direct VFD: no
                        dmalloc: no
Clear file buffers before write: yes
           Using memory checker: no
         Function Stack Tracing: no
                           GPFS: no
      Strict File Format Checks: no
   Optimization Instrumentation: no
       Large File Support (LFS): yes


</pre>
Don't worry that the C++ and Fortran languages are not enabled : Chombo uses the C interface, 
and (when we come to compile that too), so does netcdf. Assuming this is all OK, type
<pre>
> make install
</pre>
and you, after a round of compiling and copying, you 
should see that the hdf5 libraries bin,doc,include and src have appeared in $BISICLES_HOME/hdf5/serial/.
</p>
<h3>Building parallel hdf5 </h3>

Before starting, make sure that the mpi environment is in place and that you can run mpicc.
Enter hdf5/parallel/src/hdf5-1.6.10/ directory
<pre>
> cd $BISICLES_HOME/hdf5/parallel/src/hdf5-1.8.9/
</pre>
and configure hdf5, this time enabling MPI through the use of mpicc in place of gcc
<pre>
> CC=mpicc ./configure --prefix=$BISICLES_HOME/hdf5/parallel/ --enable-shared=no
</pre>
This time, configure's final report should read
<pre>

            SUMMARY OF THE HDF5 CONFIGURATION
            =================================
(...)

Languages:
----------
                        Fortran: no

                            C++: no

Features:
---------
                  Parallel HDF5: mpicc
             High Level library: yes
                   Threadsafety: no
            Default API Mapping: v18
 With Deprecated Public Symbols: yes
         I/O filters (external): deflate(zlib)
         I/O filters (internal): shuffle,fletcher32,nbit,scaleoffset
                            MPE: 
                     Direct VFD: no
                        dmalloc: no
Clear file buffers before write: yes
           Using memory checker: no
         Function Stack Tracing: no
                           GPFS: no
      Strict File Format Checks: no
   Optimization Instrumentation: no
       Large File Support (LFS): yes
</pre>
Note especially the line 'Parallel HDF5: mpicc '
</body>
Assuming this is all OK, type
<pre>
> make install
</pre>
this time, the bin,doc,include and src directories should end up  in $BISICLES_HOME/hdf5/parallel/.
</p>

<h3>Building serial netcdf </h3>
<p>
Before starting, make sure that you can run gcc,g++ and gfortran.
Enter the appropriate source directory
<pre>
> cd $BISICLES_HOME/netcdf/serial/src/netcdf-4.1.3/
</pre>
Now, netcdf needs to link hdf5 : it doesn't really matter which version but we might as
well use the one we have. So, we have 
a custom configure line
<pre>
> CC=gcc CPPFLAGS=-I$BISICLES_HOME/hdf5/serial/include/ CXX=g++ FC=gfortran LDFLAGS=-L$BISICLES_HOME/hdf5/serial/lib/  ./configure --prefix=$BISICLES_HOME/netcdf/serial   --enable-shared=no --enable-static=yes --enable-dap=no
</pre>
Next, compile, test, and install netcdf
<pre>
> make check install
</pre>
and assuming all goes well, netcdf 4.1.3 will now be installed in $BISICLES_HOME/netcdf/serial
</p>
<h3>Building parallel netcdf </h3>

<p>
So far, the only difference between parallel and serial netcdf installs 
is the link to parallel hdf5 and the use of the MPI compiler wrapper. 
Possibly, building two versions of netcdf is a waste of time. 
</p>
<p><strong>update</strong> parallel netcdf may be needed to compile parallel glimmer-CISM, otherwise it can be skipped.</p>
<pre>
> cd $BISICLES_HOME/netcdf/parallel/src/netcdf-4.1.3/
> CC=mpicc CXX=mpiCC FC=mpif90  CPPFLAGS="-DgFortran -I$BISICLES_HOME/hdf5/parallel/include/" LDFLAGS=-L$BISICLES_HOME/hdf5/parallel/lib/  ./configure --prefix=$BISICLES_HOME/netcdf/parallel  --enable-shared=no --enable-static=yes --enable-dap=no
> make  check install
</pre>

</p>
<h2><a name="petsc">Installing PETSc</a></h2>
<p>
If we're planning to use the PETSc solver interface, it's a good idea to install PETSc before building Chombo. It's likely that some version of petsc may be pre-installed on your system -- we need petsc version 3.3.4 or later,

<ol>
<li> First, download PETSc:
<pre>
git clone -b maint https://bitbucket.org/petsc/petsc petsc
</pre>

<li>  Configure petsc. This is the configure command that I used (this builds a parallel version and installs it in /usr/local/petsc/petsc-maint-mpi):
<pre>
cd petsc
./configure --download-fblaslapack=yes --download-hypre=yes -with-x=0 --with-c++support=yes --with-mpi=yes --with-hypre=yes --prefix=/usr/local/petsc/petsc-maint-mpi --with-c2html=0 --with-ssl=0
</pre> 


The petsc install system is pretty helpful and will tell you what to do
if it runs into problems (unlike, say, Chombo)

<li> Follow the instructions to make and install the library

<li> Finally, set the PETSC_DIR and PETSC_ARCH environment variables, which Chombo will use in order to find your PETSc installation. For my installation (I'm using tcsh) 
<pre>
setenv PETSC_DIR /usr/local/petsc/petsc-maint-mpi
setenv PETSC_ARCH arch-linux2-c-debug
</pre>
if using bash 
<pre>
export PETSC_DIR=/usr/local/petsc/petsc-maint-mpi
export PETSC_ARCH=arch-linux2-c-debug
</pre>
</ol>


</p>
<h2><a name="chombo">Chombo configuration</a></h2>
<p>
Next we need to set up Combo's configuration (which BISICLES will inherit automatically).
The main task here is create a file called $BISICLES_HOME/Chombo/lib/mk/Make.defs.local,
and there is version stored in this directory that should be easy enough to edit.
First, copy it into $BISICLES_HOME
<pre>
> cp $BISICLES_HOME/BISICLES/docs/Make.defs.local $BISICLES_HOME
</pre>
At the very least, you will need to  edit the line that reads
<pre> 
BISICLES_HOME=..., 
</pre>
to give the correct value. If you don't have MPI, there are a few lines to comment out. 
You might also want to tinker with the optimization flags and so on. Then create a link
so that Chombo sees Make.defs.local in the place it expects
<pre>
>ln -s $BISICLES_HOME/Make.defs.local $BISICLES_HOME/Chombo/lib/mk/Make.defs.local
</pre>

<h2><a name="python">Configuring the Python interface</a></h2>
<p>
To make use of the <a href='pythoninterface.html'>python interface</a>, you need to ensure that you have a suitable
python installation. This is usually straightforward in modern GNU/linux distributions,
since Python is so widespread. Before you can compile BISICLES with Python support, 
find out <ol>
<li>where the header file &quot;Python.h&quot; lives; /usr/include/python2.6, for example</li>
<li>what linker flags you need, for example -lpython2.6
</ol>
Now, edit the file
<pre>
$BISICLES_HOME/BISICLES/code/mk/Make.defs
</pre>
to define the variables PYTHON_INC and PYTHON_LIBS. There are several examples in the file already.
</p>

<p>when you come to compile BISICLES, you should see flags like -DHAVE_PYTHON included in the compilation</p>


<h2><a name="bisicles">Building stand-alone BISICLES</a></h2>

<p>Now we are ready to build one or more BISICLES executables. If you plan to do development
work on the code itself, you will want to build an unoptimized version to run in gdb. Run
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all
</pre>
This will build a set of Chombo libraries, and then BISICLES. Hopefully, it will complete without errors, and you
will end up with an executable called <quote>driver2d.Linux.64.g++.gfortran.DEBUG.ex</quote>. 
This one is most useful for low-level debugging of the code - if you 
are not planning to do that, there is no need for it.  If you have a serial computer only, run
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all OPT=TRUE
</pre>
to get an optimized executable called <quote>driver2d.Linux.64.g++.gfortran.DEBUG.OPT.ex</quote>. 
</p>
An optimized  parallel executable <quote>driver2d.Linux.64.mpic++.gfortran.DEBUG.OPT.MPI.ex</quote> can be built like so
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all OPT=TRUE MPI=TRUE
</pre>
For serious runs, this is the one you need. Even on workstations with a few processors (like dartagnan.ggy.bris.ac.uk) noticeable (2X-4X)  speed improvements are realized by running parallel code, and on clusters like bluecrystal or hopper.nersc.gov we have obtained 100X speedups for big enough problems (and hope to obtain 1000X speedups).
<p>
<p>
Finally, should you feel the urge, you can have a non-optimized parallel version, which can be used for hunting down low-level
bugs that crop up in parallel operation but not in serial operation. 
</p>
To build with PETSc support, add "USE_PETSC=TRUE" to your build line, e.g.
<pre>
> cd $BISICLES_HOME/BISICLES/code/exec2D
> make all OPT=TRUE MPI=TRUE USE_PETSC=TRUE
</pre>

<h2><a name="makeclean">Make clean</a></h2>
<p>On occasion it might be necessary to rebuild BISICLES entirely, rather than just those parts where a file
has changed. To do so, run
<pre>make clean</pre>
for unoptimized, serial builds, 
<pre>make clean OPT=TRUE</pre>
for optimized serial builds, and
<pre>make clean OPT=TRUE MPI=TRUE</pre>
for optimized parallel builds</p>

<h2><a name="makerealclean">Make realclean</a></h2>
<p> In the event even more houscleaning is desired, the "realclean" target does everything the "clean" target does, and additionally removes many other user-generated files, including all files with the ".hdf5" suffix (including checkpoint and plot files). 

<h2><a name="example">Running BISICLES on a simple problem</a></h2>
<p>
All the data to run <a href='http://homepages.ulb.ac.be/~fpattyn/mismip3d/welcome.html'>Frank Pattyn's MISMIP3D P075 experiment</a> is already
present. Change to the MISMIP3D subdirectory, and generate some input files from a template.
<pre>
> cd $BISICLES_HOME/BISICLES/examples/MISMIP3D
> sh make_inputs.sh
</pre>
then we are ready to go.
</p>
<h3>Running on a serial Workstation</h3>
On a serial machine, try the 3 AMR level problem (which will have
a resolution of 800 m at the grounding line, and 6.25 km far from it).
<pre>
$BISICLES_HOME/BISICLES/code/exec2D/driver2d.Linux.64.g++.gfortran.DEBUG.OPT.ex  inputs.mismip3D.p075.l1l2.l3 > sout.0 2>err.0 &
</pre>
You can watch progress by typing
<pre>
> tail -f sout.0
</pre>
and eventually, you will get a series of plot*2d.hdf5 files that you can view in visit
</p>

<h3>Running on a parallel Workstation</h3>
If you have a parallel machine, run the 5 AMR level problem (which will have
a resolution of 200 m at the grounding line, and 6.25 km far from it).
<pre>
> nohup mpirun -np 4 $BISICLES_HOME/BISICLES/code/exec2D/driver2d.Linux.64.mpic++.gfortran.DEBUG.OPT.MPI.ex inputs.mismip3D.p075.l1l2.l5 &
</pre>
replace -np 4 with the appropriate count for your machine: this is usually <strong>not</strong> the number
of CPU cores because these will typically share some resources. A good guess is 
the number of memory buses. Ideally, do a series of scaling experiments
to come up with a graph like this one 
</p>
<img src="porthos.png" alt='scaling with CPU count on porthos.ggy.bris.ac.uk, 5 level MISMIP3D problem'/>
<p>
See also the <a href='sites.html'>Site specific notes</a>
</p>

<p>
You can watch progress by typing
<pre>
> tail -f pout.0
</pre>
and eventually, you will get a series of plot*2d.hdf5 files that you can view in visit
</p>
<h3>Running on a cluster</h3>

See the <a href='sites.html'>Site specific notes</a>

<h3> Using the PETSc solvers</h3> 
The examples presented in this tutorial all use the default Chombo geometric multigrid (GMG) solvers. If you find that convergence of the velocity solver is stalling (or even diverging), you might want to try a PETSc algebraic multigrid (AMG) solver. To switch to the PETSc solvers,
<ol>
<li> You will need a <pre>.petsrc</pre> file to tell the PETSc solvers what to do. There is an example one in the exec2D directory which is set to parameters which have worked well for us. For example, it uses the <a href='http://computation.llnl.gov/casc/linear_solvers/sls_hypre.html'>HYPRE </a> BoomerAMG solver. 

<li> Edit your BISICLES input file. There are two ways to use PETSC:
<ul>
<li> As a bottom solver in the Chombo GMG solver. (if you don't know what that means, don't worry -- suffice it to say that in many cases, this option occupies the middle ground between the Chombo native GMG solvers and a full AMR AMG solver.). Assuming you are using the JFNKSolver for your velocity solve, add the following line to your inputs file:
<pre>
JFNKSolver.bottom_solver_type = 1
</pre>

<li> If that doesn't help, then you're ready for the full AMR PETSc solver. Assuming once again that you're using the JFNKSolver, then change 
<pre> 
JFNKSolver.solverType = 0
</pre>
to 
<pre> 
JFNKSolver.solverType = 4
<\pre>

<\ul>

</ol>

<h3><a name='nctoamr'>Compiling and using the file tools</a></h3>
<p>
To compile the file tools you might need to edit file $BISICLES_HOME/BISICLES/code/mk/Make.defs. 
If you have installed BISICLES (and netcdf in particular) following this guide, there should be no need, but otherwise
make sure that the variable NETCDF_HOME points to the parent directory of the netcdf include and lib directories.
There is little point in building parallel file tools, so type
<pre>
cd $BISICLES_HOME/BISICLES/code/filetools/
make all
</pre>
now, say you have a NetCDF file called data.nc, which contains 2D fields named thk and topg arranged on a regular
grid (with equal spacing in x and y), and must also contain a 1D field called x, you can run
<pre>
$BISICLES_HOME/BISICLES/code/filetools/nctoamr2d.Linux.64.g++.gfortran.DEBUG.ex data.nc data.2d.hdf5 thk topg
</pre>  
to obtain a file called data.2d.hdf5 that BISICLES can read as (say) thickness and topography data. 
</p>
There are a number of other <a href='filetools.html'>file tools</a>.

<h3><a name='control'>Compiling the control problem code</a></h3>
<p>
  The control problem code is used to solve an optimization problem (inverse problem, or ill-posed problem),
  using the optimal control methods described by (e.g) MacAyeal (1993) Journal of Glaciology, vol 39 p 91
  and elsewhere. It lives somewhat apart from the rest on BISICLES, and has its own executable
 <pre>
   cd $BISICLES_HOME/BISICLES/control
   make all OPT=TRUE #compile a serial optimized version
   make all OPT=TRUE MPI=TRUE #compile a parallel optimized version
</pre>
 There is a <a href='pineislandcontrol.html'>Pine Island Glacier example</a> which shows how to run the control problem
 and what to do with its results.
</p>


</body>
</html>
